\section{Interoperating with Existing Library Constructs}
\label{sec:library}

There is a clear parallel between the manual lambda specializations that are already in the Java Standard Library and thus cannot be eliminated and the specialized constructs in the Scala Standard Library, which cannot be replaced by a compiler plugin. Project Valhalla brings the ability to specialize generics to Java, while miniboxing brings a new compilation scheme for Scala generics. What is common between the two cases is the hard requirement that the new transformations work well with the existing constructs, which use different compilation schemes. This is the problem of interoperating with existing libraries.

In this section we will show how performance regressions occur when miniboxed code interacts with the Scala standard library, which uses either erased generics or the original specialization transformation. To counter these performance regressions, we show three approaches to efficiently bridge the gap between the miniboxing and specialization compilation schemes. Although this section mostly focuses on the interoperation between miniboxing and specialization, the techniques are general and can be applied to Java lambdas and Valhalla as well.

\subsection{The Interoperation Problem}

When interacting with the library from miniboxed code, the programmers forget the fact that library constructs, such as tuples and functions, do not share the same compilation scheme. Thus, they expect the same performance and flexibility as when using miniboxed classes. However, calling specialized code from miniboxed methods and vice-versa is not trivial:

\begin{lstlisting-nobreak}
 def spec[@specialized T](t: T): String = t.toString
 def mbox[@miniboxed T](t: T): String = spec(t)
\end{lstlisting-nobreak}

The translated low-level code is:

\begin{lstlisting-nobreak}
 def spec(t: Object): String = t.toString
 def spec`_I`(t: int): String = Integer(t).toString
 def spec`_J`(t: long): String = Long(t).toString
 ... // other 7 specialized variants
 def mbox(t: Object): String = `spec(t)`
 def mbox`_M`(T_Type: byte, t: long): String = `???`
\end{lstlisting-nobreak}

The reference-based |mbox| and |spec| methods can directly call each other, since there is a 1 to 1 correspondence. The problem is that, unlike these two methods, none of the specialized variants have a 1 to 1 correspondence. This only leaves the reference-based methods as candidates for the direct interoperation between miniboxing and specialization.

Although it may seem like |mbox_M| could directly invoke |spec_J|, since the argument types match, this would be incorrect, as the value |t| in |mbox_M| can be any primitive type, encoded as a long, whereas |t| in |spec_J| can only be a long integer. Thus, if we were to call |spec_J| from |mbox_M| passing an encoded boolean, instead of returning either ``true'' or ``false'', it would return the encoded value of the boolean.

The |mbox_M| method is in possession of one more piece of information: |T_Type|, the type byte describing the encoded primitive type. The type byte allows the miniboxed method to choose which of its specialized counterparts to call:

\begin{lstlisting-nobreak}
 def mbox`_M`(T_Type: byte, t: long): String =
   T_Type match {
     case INT     => spec_I(minibox2int(t))
     case LONG => spec_M(minibox2long(t))
     ...
   }
\end{lstlisting-nobreak}

Although this indirect approach seems to work and can easily be automated, it is actually a  step back in the wrong direction: the miniboxing transformation would be introducing extra overhead without offering the programmer any feedback on how and why this happens. Furthermore, when multiple type parameters are specialized, all $10^N$ possible combinations would have to be added to the match, making it very large. This is likely to confuse the Java Virtual Machine inlining heuristics, causing severe performance regressions that are difficult to debug.

It may seem like the other way around would be easier: allowing specialized code to call miniboxed methods without performing a switch. However this is not the case because specialization is not aware of miniboxing. Therefore, even when calling miniboxed methods, specialization resorts to invoking the generic version, boxing the arguments and unboxing the returned value.

With this in mind, our decision was to go with simplicity and symmetry: the bridge between miniboxing and specialization goes through boxing. To allow transparency, miniboxing issues performance advisories about specialized code that should be miniboxed:

\begin{lstlisting-nobreak-nolang}
scala>  def mbox[@miniboxed T](t: T): String = spec(t)
<console>:8: warning: Although the type parameter T of method spec is specialized, miniboxing and specialization communicate among themselves by boxing (thus, inefficiently) on all classes other than as FunctionX and TupleX. If you want to maximize performance, consider switching from specialization to miniboxing:
        def mbox[@miniboxed T](t: T): String = spec(t)
                                                            ^
\end{lstlisting-nobreak-nolang}

This solution works well with most of the code that lies within the programmer's control, including for the case where 3rd party libraries distribute both a specialized and a miniboxed version. However, the one library which cannot have multiple versions and happens to use specialization is the Scala standard library. The two most wide-spread constructs affected by this are Tuples and Functions, both of which are specialized. This makes the following function a worst-case scenario for vanilla miniboxing:

\begin{lstlisting-nobreak}
 def tupleMap[@miniboxed T,
                  @miniboxed U](tup: (T, T), f: T => U) =
   (f(tup._1), f(tup._2))
\end{lstlisting-nobreak}

Despite the annotations, with the vanilla miniboxing transformation, all versions of the |tupleMap| method use reference-based tuple accessors and function applications, leading to slow paths irreversibly creeping into miniboxed code. For many applications, this is a no-go, so our task was to eliminate these slowdowns. In the following subsection we present three possible approaches and show where each works best.

\subsection{Eliminating the Interoperation Overhead}

We show three approaches to eliminating the boxing overhead when calling specialized code from miniboxed classes or methods.

\subsubsection{Accessors.} The simplest answer to the problem of inter-operating with specialization is to switch on the type byte, as shown previously. To avoid confusing the Java Virtual Machine inlining heuristics, we can extract the operation into a static method, that we call separately. This approach needs to be implemented both for accessors, allowing the specialized values to be extracted directly into the miniboxed encoding and for constructors, allowing miniboxed code to instantiate specialized classes without boxing. This is the approach taken for |Tuples| (\S\ref{sec:tuples});

\subsubsection{Transforming objects.} The accessors approach allows us to pay a small overhead with each access. This is a good trade-off when the constructs are only accessed a couple of times during their lifetime, which is the case for tuples. In other cases, such as functions, the |apply| method is presumably called many times during the object lifetime, making it worthwhile to completely eliminate the overhead. In this case, a better approach is to replace the |Function| objects by |MiniboxedFunction|s, introducing conversions between them where necessary. This way, the |apply| method exposed by |MiniboxedFunction| can be called directly, and this can compensate for a potentially greater cost of constructing the |MiniboxedFunction| object. This way, switching on the type bytes is done only once, when converting the function, and then amortizes over the function lifetime (\S\ref{sec:functions});

\subsubsection{New API.} In some cases, the API and guarantees are hardcoded into the platform. This is the case for the Scala |Array| class, for which the original miniboxing plugin chose the accessors approach \cite{miniboxing}. However, a better tradeoff is achieved by defining a new |MbArray| class with a similar API. This approach will be briefly mentioned in the Arrays subsection (\S\ref{sec:mbarrays}).

The next sections discuss the three methods above.

\subsection{Tuple Accessors}
\label{sec:tuples}

The Scala programming language offers a very concise and pleasant syntax for library tuples, allowing users to write |(3,5)| instead of the desugared |new Tuple2[Int, Int](3,5)|. Similarly, it allows programmers to write |(Int, Int)| instead of |Tuple2[Int, Int]|. If we were to introduce miniboxed tuples, we would not be able to use the syntactic sugar to express programs, losing the support of many programmers. Instead, a better choice is to find way to efficiently access specialized Scala tuples, without boxing values.

Although we don't have statistically significant data, our experience suggests that |Tuple| classes have their components accessed only a few times during their life. Therefore, both for compatibility reasons and to avoid costly conversions, we decided to allow the |Tuple| class to remain unchanged, instead focusing on providing accessors and constructors that use the miniboxed encoding.

\subsubsection{The optimized tuple accessors} are written by hand and are explicitly given the type byte:

\begin{lstlisting-nobreak}
 def tuple1_accessor_1[T](T_Tag: Byte, tp: Tuple1[T]) =
   T_Tag match {
     case INT =>
       // the call to _1 will be rewritten to a call
       // to the specialized variant _1_I, which
       // returns the integer in the unboxed format:
       int2minibox(tp.asInstanceOf[Tuple1[Int]]._1)
     ...
   }
\end{lstlisting-nobreak}

Once the tuple is cast to a |Tuple1[Int]|, the specialization transformation kicks in and transforms the call to |_1| into a specialized call to |_1_I|, the integer variant. Since the |int2minibox| conversion also takes an unboxed integer, the overhead of boxing is completely eliminated.

\subsubsection{The specialized constructors} are motivated by two observations: (1) allocating tuples in the miniboxed code without special support requires boxing and, even worse (2) the tuples created use the reference-based variant of the specialized class, thus voiding the benefits of having added tuple accessors. The code for the tuple constructors is also written by hand and is very similar to the accessor code: it dispatches on the type tags to create tuples of primitive types, which specialization can rewrite to the specialized variants.

\subsubsection{Closing the cycle} is automatically done by the miniboxing plugin when encountering a tuple access followed by a conversion to the miniboxed representation or when the tuple constructor is invoked with all the arguments being transformed from the miniboxed representation to the boxed one. There are two reasons this step needs to be automated:

\begin{compactitem}
 \item By default, programmers do not have access to the type bytes directly, as this would allow them to introduce unsoundness in the type system (they can access the type byte in a read-only fashion, using miniboxing reflection, but this is outside the scope of this paper);
 \item One of the reasons tuples are useful is their great integration with the language, allowing a very concise syntax. Asking programmers to use anything other than this syntax would be as bad as developing our own, no-syntax-sugar miniboxed tuple.
\end{compactitem}

With these three changes, benchmarks show a 2x speedup when accessing tuples and a 5\% slowdown compared to the equivalent code which accesses the tuples directly. The benchmark we used was a tuple quicksort algorithm (\S\ref{sec:bench}).

With the three elements above, accessors, constructors and the automatic transformation we create a direct bridge between specialized tuples and miniboxed classes. Unfortunately, as we've seen before, adding such accessors has to be a carefully-weighted, context-specific decision, so automating it would not provide much benefit. For example, this choice would not be suitable for functions.

\subsection{Functions}
\label{sec:functions}

The same syntax challenge applies to functions. In our early experiments on transforming the Scala collections hierarchy using the miniboxing transformation \cite{miniboxing-linkedlist}, we were proposing an alternative |MbFunction| and desugaring by hand. The performance obtained was good, but desugaring by hand was too tedious. Later on, we received a suggestion from the Scala community (thanks to Alex Nedelcu!) stating that, since functions in Scala are specialized, we should be able to interface directly, thus benefiting from the desugaring build into Scala without paying for the boxing overhead.

The first approach was using accessors, but we soon learned that switching on as many as 3 type bytes with each function application incurs a significant overhead. Instead, we decided to fast-path the function application while turning function creation into a slow path. The reasoning was that functions are usually created once but applied many times, so any delay in the creation amortizes over many applications. Indeed, the benchmarks show speedups from 2x to 14x when using miniboxed functions. The key to this is replacing the Scala |FunctionsX| by |MbFunctionX|, where |X|, the function arity, is either 0, 1 or 2 (the functions with greater arity are not specialized, due to the bytecode explosion problem):

\begin{lstlisting-nobreak}
 def choice[@miniboxed T](seed: Int): `(T, T) => T` =
   (t1: T, t2: T) => if (seed % 2 == 0) t1 else t2

 val function: `Int => Int` = choice(Random.nextInt)
 List((1,2), (3,4), (5,6)).map(`function`)
\end{lstlisting-nobreak}

Explaining how the transformation occurs in detail is beyond the scope of this paper and has been studied in previous work \cite{ldl,ildl-tech}. However, there are four ways in which slow paths can occur as a result of the transformation:
\begin{compactitem}
  \item Converting |FunctionX| objects to |MbFunctionX|;
  \item Converting |MbFunctionX| objects to |FunctionX|;
  \item Applying the |FunctionX| object extracted from an |MbFunctionX|;
  \item Applying the |MbFunctionX| object generated from a |FunctionX|;
\end{compactitem}

Let us look at the result of the translation:

\begin{lstlisting-nobreak}
 def choice(seed: int): `MbFunction2` =
   new AbstractMbFunction2_LL {
     def apply(t1: Object, t2: Object) = ...
     val functionX: Function2 = ...
   }
 def choice`_M`(T_Type: byte, seed: int): `MbFunction2` =
   new AbstractMbFunction2_MJ {
     def apply_MJ(..., t1: long, t2: long) = ...
     val functionX: Function2 = ...
   }

 val function: `MbFunction2` = choice_M(...)
 List((1,2), (3,4), (5,6)).map(`function.functionX`)
\end{lstlisting-nobreak}

What we can see from the translation is that anonymous Scala |Function|s are automatically transformed to |MbFunction|s and that |MbFunction|s have a dual nature: they contain both the miniboxed code for the function and the specialized |Function| equivalent. Therefore, the cost of converting from one function representation to the other is as low as accessing a field (the function object in the |functionX| field also holds a reference to the outer |MbFunction|). This addresses the first two possible slowdowns.

The third overhead is addressed by allowing the object in the |functionX| field to directly call the miniboxed function with very little overhead, by converting from unboxed primitive values to miniboxed values and back, without then need for boxing. This imposes a minimum overhead when a function declared in a miniboxed scope is passed to a library that did not undergo the miniboxing transformation, such as the Scala library (the last line in the example). Therefore, passing functions from miniboxed code back to generic or specialized code is done efficiently.

Finally, the last type of slowdown occurs when a miniboxed function is expected, but the incoming function was defined outside the miniboxed code, resulting in a plain |FunctionX| object. This is the only case where we actually need to perform a switch, extracting the specialized version of the class and wrapping it into a miniboxed function. Since callbacks from the Scala library into miniboxed code are not common (if they exist at all), the slow path of executing this switch rarely occurs.

To recap the four slow paths that can occur:

\begin{compactitem}
  \item Converting |FunctionX| objects to |MbFunctionX|;
  \item Converting |MbFunctionX| objects to |FunctionX|;
  \begin{compactitem}
    \item Are done by accessing a field in either class, resulting in fast execution. The only case where a slow path occurs is when a function is passed from non-miniboxed code, which is a rare occurrence and is guarded by performance advisories.
  \end{compactitem}
  \item Applying the |FunctionX| object extracted from |MbFunctionX|;
  \begin{compactitem}
    \item Occurs without boxing, since the |FunctionX| object directly converts arguments to the miniboxed encoding.
  \end{compactitem}
  \item Applying the |MbFunctionX| object generated from |FunctionX|;
  \begin{compactitem}
    \item Occurs without boxing, since we match the specialized variant of the |FunctionX| and only convert the miniboxed encoding to unboxed generics.
  \end{compactitem}
\end{compactitem}

Therefore, the |MbFunction| encoding allows us to call functions without boxing their values and offsets the slow path to a very remote use case, which is also guarded by performance advisories. The next subsection looks into arrays.

