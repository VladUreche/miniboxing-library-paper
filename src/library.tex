\section{Functions and Tuples}
\label{sec:library}

In this section we will show how slow paths occur when miniboxed code interacts with the Scala standard library, which uses either erased generics or the original specialization transformation, which generates 10 variants per type parameter. We show three approaches to avoiding the slow path and show how each of them fits with the library entities.

When interacting with the library from miniboxed code, the programmers are aware functions and tuples are specialized, so they expect to have automatic and flawless performance when using them. However, calling specialized code from miniboxed code and vice-versa is not trivial:

\begin{lstlisting-nobreak}
 def mbox[@miniboxed T](t: T): String = spec(t)
 def spec[@specialized T](t: T): String = t.toString
\end{lstlisting-nobreak}

The translated low-level code is:

\begin{lstlisting-nobreak}
 def mbox(t: Object): String = `spec(t)`
 def mbox`_J`(T_Type: byte, t: long): String = `???`
 def spec(t: Object): String = t.toString
 def spec`_I`(t: int): String = Integer(t).toString
 def spec`_J`(t: long): String = Long(t).toString
 ... // other 7 specialized variants
\end{lstlisting-nobreak}

The generic |mbox| method calls |spec|, since there is a 1 to 1 correspondence between them. This is not the case for |mbox_J| and the nine specializations of method |spec|. And while it may seem like |mbox_J| could directly invoke |spec_J|, since the argument type matches, this would be incorrect, as |t| in |mbox_J| can be any primitive type, including boolean, whereas |spec_J| would treat the encoding as a long integer (thus, instead of returning the string ``true'' or ``false'', it would return the encoded value of the boolean).

The |mbox_J| method is in possession of one more piece of information: |T_Type|, the type byte describing the encoded primitive type. In order to call the correct specialized method, |mbox_J| could switch on the type byte and perform the correct conversion:

\begin{lstlisting-nobreak}
 def mbox`_J`(T_Type: byte, t: long): String =
   T_Type match {
     case INT => spec_I(minibox2int(t))
     case LONG => spec_J(minibox2long(t))
     ...
   }
\end{lstlisting-nobreak}

Although this approach seems to work, it takes a step back on the transparency scale: the miniboxing transformation would be introducing extra work without offering the programmer any feedback on how and why this happened. Furthermore, multiple type bytes may be involved in the switch, generating a combinatorial explosion which is likely to confuse the Java Virtual Machine heuristics for inlining, leading to sudden (and difficult to debug) slow paths.

It may seem like the other way around would be much easier, allowing specialized code to call miniboxed code without performing a switch. However this is not the case since the specialization transformation is not aware of miniboxing. Therefore, when calling miniboxed methods, specialization resorts to invoking the generic version, boxing the arguments and unboxing the returned value.

With these aspects in mind, our decision was to go with simplicity and symmetry: both miniboxing and specialization go through boxing when calling each other's transformed code. To allow transparency, miniboxing issues performance advisories about specialized code that should be miniboxed:

\begin{lstlisting-nobreak}
 def mbox`_J`(T_Type: byte, t: long): String =
   // performance advisory: forward warning
   spec(minibox2box(T_Type, t))
\end{lstlisting-nobreak}

This solution works well with most of the code that lies within the programmer's control, including for the case where 3rd party libraries distribute either a specialized or a miniboxed version. However, the one library which cannot have multiple versions and happens to use specialization is the Scala standard library:

\begin{lstlisting-nobreak}
 def tupleMap[@miniboxed T, @miniboxed U](tup: (T, T), f: T => U): (U, U) =
   (f(tup._1), f(tup._2))
\end{lstlisting-nobreak}

Despite the miniboxing annotations, all versions of the |tupleMap| method use the erased generic versions of the tuple acessors and the function application, leading to slow paths irreversibly creeping into miniboxed code. For many applications, this is a no-go, so our task was to eliminate these cases. In the following subsection we present three possible approaches and show where each works best.

\subsection{Eliminating Inter-operation Overhead}

We now discuss three approaches to eliminating the boxing overhead when calling specialized code from miniboxed classes and/or methods.

\subsubsection{Accessors.} The simplest answer to the problem of inter-operating with specialization is to switch on the type byte, as shown above. To avoid confusing the Java Virtual Machine inlining heuristics, we can extract the operation into a static method, that we call separately. This approach needs to be implemented both for accessors, allowing the specialized values to be extracted directly into the miniboxed encoding and for constructors, allowing miniboxed code to instantiate specialized classes without boxing. This is the approach taken for |Tuples| (\S\ref{sec:tuples});

\subsubsection{Wrapping objects.} The accessors approach allows us to pay a small (but non-zero) overhead with each access, which is a good tradeoff when the values accessed are only accessed a couple of times during the lifetime of the object. In other cases, such as functions, the |apply| method is expected to be called many times during the object lifetime, making the overhead of the |apply| method undesirable. In this case, another approach is to introduce a wrapper object, in this case |MiniboxedFunction| which exposes a miniboxed |apply| method and does not need to switch on the type byte in order to call the wrapped function: the switch is done when wrapping the function and is a one-time cost, which then amortizes over many function applications (\S\ref{sec:functions});

\subsubsection{New API.} In some cases, the API and guarantees are hardcoded into the compiler. This is the case for the Scala |Array| class, for which the miniboxing plugin chose the Accessors approach \cite{miniboxing}. However, several limitations of Scala arrays can be eliminated by defining a new |MbArray| class. This approach will be discussed in detail in the Arrays subsection (\S\ref{sec:mbarrays}).

The next section discusses three Scala library classes for which the miniboxing plugin uses the approaches above.

\subsection{Tuples}
\label{sec:tuples}

The Scala programming language offers a very concise and pleasant syntax for library tuples, allowing users to write |(3,5)| instead of the desugared |new Tuple2[Int, Int](3,5)|. Similarily, it allows programmers to write |(Int, Int)| instead of |Tuple2[Int, Int]|. If we were to introduce miniboxed tuples, we would not be able to use the syntactic sugar to express programs, losing the support of many programmers. Instead, we're looking into ways to access specialized tuples efficiently, without boxing values.

Although we haven't measured rigorously, our experience suggests that |Tuple| classes are usually created just to have their components accessed a few times during their life. Therefore, both for compatibility reasons and to avoid costly conversions, we decided to allow the |Tuple| class to remain unchanged, instead focusing on providing accessors and constructors to provide compatibility.

Our measurements show that we obtain a 2x speedup using the accessors approach and we are within 5\% of the specialized (and monomorphic) code for a larger tuple quicksort benchmark.

\subsection{Functions}
\label{sec:functions}

The same syntax challenge applies to functions. In our early experiments on transforming the Scala collections hierarchy using the miniboxing transformation \cite{miniboxing-linkedlist}, we were proposing an alternative |MbFunction| and desugaring by hand. The performance obtained was good, but desugaring by hand was too tedious. Later on, we received a suggestion from the Scala community (thanks to Alex Nedelcu!) stating that, since functions in Scala are specialized, we should be able to interface directly, thus benefiting from the desugaring build into Scala without paying for the boxing overhead.

The first approach was using accessors, but we soon learned that switching on as many as 3 type bytes with each function application incurs a significant overhead. Instead, we decided to fast-path the function application while turning function creation into a slow path. The reasoning was that functions are usually created once but applied many times, so any delay in the creation amortizes over many applications. Indeed, the benchmarks show speedups from 2x to 14x when using miniboxed functions. The key to this is replacing the Scala |FunctionsX| by |MbFunctionX|, where |X|, the function arity, is either 0, 1 or 2 (the functions with greater arity are not specialized, due to the bytecode explosion problem):

\begin{lstlisting-nobreak}
 def choice[@miniboxed T](seed: Int): `(T, T) => T` =
   (t1: T, t2: T) => if (seed % 2 == 0) t1 else t2

 val function: `Int => Int` = choice(Random.nextInt)
 List((1,2), (3,4), (5,6)).map(`function`)
\end{lstlisting-nobreak}

Explaining how the transformation occurs in detail is beyond the scope of this paper and has been studied in previous work \cite{ldl,ildl-tech}. However, there are four ways in which slow paths can occur as a result of the transformation:
\begin{compactitem}
  \item Converting |FunctionX| objects to |MbFunctionX|;
  \item Converting |MbFunctionX| objects to |FunctionX|;
  \item Applying the |FunctionX| object extracted from an |MbFunctionX|;
  \item Applying the |MbFunctionX| object generated from a |FunctionX|;
\end{compactitem}

Let us look at the result of the translation:

\begin{lstlisting-nobreak}
 def choice(seed: int): `MbFunction2` =
   new AbstractMbFunction2_LL {
     def apply(t1: Object, t2: Object) = ...
     val functionX: Function2 = ...
   }
 def choice`_J`(T_Type: byte, seed: int): `MbFunction2` =
   new AbstractMbFunction2_JJ {
     def apply_JJ(..., t1: long, t2: long) = ...
     val functionX: Function2 = ...
   }

 val function: `MbFunction2` = choice_J(...)
 List((1,2), (3,4), (5,6)).map(`function.functionX`)
\end{lstlisting-nobreak}

What we can see from the translation is that anonymous Scala |Function|s are automatically transformed to |MbFunction|s and that |MbFunction|s have a dual nature: they contain both the miniboxed code for the function and the specialized |Function| equivalent. Therefore, the cost of converting from one function representation to the other is as low as accessing a field (the function object in the |functionX| field also holds a reference to the outer |MbFunction|). This addresses the first two possible slowdowns.

The third overhead is addressed by allowing the object in the |functionX| field to directly call the miniboxed function with very little overhead, by converting from unboxed primitive values to miniboxed values and back, without then need for boxing. This imposes a minimum overhead when a function declared in a miniboxed scope is passed to a library that did not undergo the miniboxing transformation, such as the Scala library (the last line in the example). Therefore, passing functions from miniboxed code back to generic or specialized code is done efficiently.

Finally, the last type of slowdown occurs when a miniboxed function is expected, but the incoming function was defined outside the miniboxed code, resulting in a plain |FunctionX| object. This is the only case where we actually need to perform a switch, extracting the specialized version of the class and wrapping it into a miniboxed function. Since callbacks from the Scala library into miniboxed code are not common (if they exist at all), the slow path of executing this switch rarely occurs.

To recap the four slow paths that can occur:

\begin{compactitem}
  \item Converting |FunctionX| objects to |MbFunctionX|;
  \item Converting |MbFunctionX| objects to |FunctionX|;
  \begin{compactitem}
    \item Are done by accessing a field in either class, resulting in fast execution. The only case where a slow path occurs is when a function is passed from non-miniboxed code, which is a rare occurrence and is guarded by performance advisories.
  \end{compactitem}
  \item Applying the |FunctionX| object extracted from |MbFunctionX|;
  \begin{compactitem}
    \item Occurs without boxing, since the |FunctionX| object directly converts arguments to the miniboxed encoding.
  \end{compactitem}
  \item Applying the |MbFunctionX| object generated from |FunctionX|;
  \begin{compactitem}
    \item Occurs without boxing, since we match the specialized variant of the |FunctionX| and only convert the miniboxed encoding to unboxed generics.
  \end{compactitem}
\end{compactitem}

Therefore, the |MbFunction| encoding allows us to call functions without boxing their values and offsets the slow path to a very remote use case, which is also guarded by performance advisories. The next subsection looks into arrays.

